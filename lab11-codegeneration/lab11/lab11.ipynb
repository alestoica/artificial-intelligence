{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f405aa4-9ca3-46ef-9994-f608ba6e8fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algoritmi pe siruri de caractere\n",
      "Algoritmi pe siruri de caractere\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import choice\n",
    "from math import sqrt\n",
    "from transformers import RobertaTokenizer, RobertaModel, GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# KMeans clustering class definition\n",
    "class KMeans():\n",
    "    def __init__(self, num_centroids) -> None:\n",
    "        self.num_centroids = num_centroids\n",
    "        self.centroids = []\n",
    "\n",
    "    def select_centroids(self, data):\n",
    "        indices = [i for i in range(data.shape[0])]\n",
    "        centroid_indices = choice(indices, self.num_centroids, replace=False)\n",
    "        self.centroids = [data[i] for i in centroid_indices]\n",
    "\n",
    "    def calculate_distance(self, point1, point2):\n",
    "        squared_diff = [(point1[0, i] - point2[0, i]) ** 2 for i in range(point1.shape[1])]\n",
    "        distance = sqrt(sum(squared_diff))\n",
    "        return distance\n",
    "\n",
    "    def find_closest_centroid(self, point):\n",
    "        min_distance = self.calculate_distance(point, self.centroids[0])\n",
    "        closest_centroid_idx = 0\n",
    "\n",
    "        for i in range(1, len(self.centroids)):\n",
    "            distance = self.calculate_distance(point, self.centroids[i])\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_centroid_idx = i\n",
    "\n",
    "        return closest_centroid_idx\n",
    "\n",
    "    def calculate_centroid_sum(self, data, assignments, centroid_idx):\n",
    "        return sum([data[i] for i in range(data.shape[0]) if assignments[i] == centroid_idx])\n",
    "\n",
    "    def count_points_assigned_to_centroid(self, assignments, centroid_idx):\n",
    "        return assignments.count(centroid_idx)\n",
    "\n",
    "    def train(self, data):\n",
    "        self.select_centroids(data)\n",
    "        converged = False\n",
    "\n",
    "        while not converged:\n",
    "            assignments = [self.find_closest_centroid(data[i]) for i in range(data.shape[0])]\n",
    "            max_centroid_shift = -1\n",
    "\n",
    "            for centroid_idx in range(self.num_centroids):\n",
    "                if self.count_points_assigned_to_centroid(assignments, centroid_idx) != 0:\n",
    "                    new_centroid = self.calculate_centroid_sum(data, assignments, centroid_idx) / self.count_points_assigned_to_centroid(assignments, centroid_idx)\n",
    "                else:\n",
    "                    new_centroid = self.centroids[centroid_idx]\n",
    "                distance = self.calculate_distance(self.centroids[centroid_idx], new_centroid)\n",
    "\n",
    "                if distance > max_centroid_shift:\n",
    "                    max_centroid_shift = distance\n",
    "                self.centroids[centroid_idx] = new_centroid\n",
    "\n",
    "            if max_centroid_shift < 0.05:\n",
    "                converged = True\n",
    "\n",
    "    def predict(self, data):\n",
    "        return [self.find_closest_centroid(point) for point in data]\n",
    "\n",
    "# Functions to get embeddings using CodeBERT\n",
    "def get_embedding_for_code_snippet(code_snippet) -> np.array:\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "    model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "    inputs = tokenizer(code_snippet, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "    return embedding\n",
    "\n",
    "# Function to read code snippet from a file\n",
    "def read_code_snippet(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        code = file.read()\n",
    "    return code\n",
    "\n",
    "# Function to read CSV file\n",
    "def read_from_file(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Function to get the scope from a CSV file based on index\n",
    "def get_scope(file_path, scope_index):\n",
    "    dataframe = read_from_file(file_path)\n",
    "    scopes = list(set(dataframe[\"scop\"]))\n",
    "    return scopes[scope_index]\n",
    "\n",
    "# Function to generate and train the KMeans classifier\n",
    "def generate_classifier(file_path):\n",
    "    dataframe = read_from_file(file_path)\n",
    "    code_paths = dataframe[\"code_snippet\"]\n",
    "    scopes = dataframe[\"scop\"]\n",
    "    num_centroids = len(set(scopes))\n",
    "\n",
    "    code_snippets = [read_code_snippet(file) for file in code_paths]\n",
    "    embeddings = [get_embedding_for_code_snippet(code) for code in code_snippets]\n",
    "\n",
    "    classifier = KMeans(num_centroids)\n",
    "    classifier.train(np.array(embeddings))\n",
    "    return classifier\n",
    "\n",
    "# Function to generate the scope for a given code\n",
    "def generate_code_scope(code, classifier):\n",
    "    embedding = get_embedding_for_code_snippet(code)\n",
    "    return classifier.predict(np.array([embedding]))[0]\n",
    "\n",
    "# Example usage\n",
    "classifier = generate_classifier(\"codes.csv\")\n",
    "\n",
    "code = \"\"\"def factorial(n):\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n-1)\n",
    "\"\"\"\n",
    "scope_index = generate_code_scope(code, classifier)\n",
    "code_scope = get_scope(\"codes.csv\", scope_index)\n",
    "print(code_scope)\n",
    "\n",
    "code = \"\"\"def fibonacci(n):\n",
    "    a, b = 0, 1\n",
    "    for _ in range(n):\n",
    "        a, b = b, a + b\n",
    "    return a\n",
    "\"\"\"\n",
    "scope_index = generate_code_scope(code, classifier)\n",
    "code_scope = get_scope(\"codes.csv\", scope_index)\n",
    "print(code_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0653f56f-cac9-43b6-a872-45d6460a97f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python's max_number is a very important property of Python. Not only does it determine the value of a number, but also whether it is actually a valid number.\n",
      "\n",
      "The above method looks up the minimum and maximum number to get a list of all possible integers. Here's an example:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.python.org/reference/python_string_types.html#reverse_string and http://www.python.org/reference/python_string_types.html#reverse_string is a variant of reverse_tuple with the same semantics as described in http://support.python.org/download/index.html and http://support.python.org/download\n"
     ]
    }
   ],
   "source": [
    "# Function to generate comments using GPT-2\n",
    "def generate_comments_with_model(code):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "    prompt = \"Generate a detailed comment for the following Python function:\\n\\n\" + code + \"\\n\\nComment:\\n\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    outputs = model.generate(inputs, max_length=150, num_return_sequences=1, temperature=0.8, do_sample=True, top_k=50)\n",
    "    generated_comment = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    comment_start = generated_comment.find(\"Comment:\") + len(\"Comment:\")\n",
    "    if comment_start == -1:\n",
    "        return \"No comment generated\"\n",
    "    comment = generated_comment[comment_start:].strip()\n",
    "\n",
    "    return comment\n",
    "\n",
    "python_code_without_comments = \"\"\"\n",
    "def find_maximum(numbers):\n",
    "    max_number = numbers[0]\n",
    "    for number in numbers:\n",
    "        if number > max_number:\n",
    "            max_number = number\n",
    "    return max_number\n",
    "\"\"\"\n",
    "\n",
    "generated_comment = generate_comments_with_model(python_code_without_comments)\n",
    "print(generated_comment)\n",
    "\n",
    "python_code_without_comments = \"\"\"\n",
    "def reverse_string(s):\n",
    "    reversed_s = ''\n",
    "    for char in s:\n",
    "        reversed_s = char + reversed_s\n",
    "    return reversed_s\n",
    "\"\"\"\n",
    "\n",
    "generated_comment = generate_comments_with_model(python_code_without_comments)\n",
    "print(generated_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81b1dc2f-ca73-49eb-8619-cb2b4d21af43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 - Generated Code:\n",
      "Write a Python function to verify if a number is prime.\n",
      "\n",
      "def isPrime(n):\n",
      "    if n < 2:\n",
      "        return False\n",
      "    for i in range(2, int(math.sqrt(n))+1):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "print(isPrime(10))\n",
      "\n",
      "# This is a test function.\n",
      "# It will print the number of primes below 10.\n",
      "\n",
      "# This is a test function.\n",
      "\n",
      "Example 2 - Generated Code:\n",
      "Write a Python function to find the factorial of a number.\n",
      "\n",
      "def factorial(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    return n * factorial(n-1)\n",
      "\n",
      "print(factorial(10))\n",
      "\n",
      "# This is a Python function that takes a number and returns the factorial of it.\n",
      "\n",
      "def factorial(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    return n * factorial(n-1)\n",
      "\n",
      "print(factorial(10))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Load the tokenizer and model for code generation.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Daoguang/PyCodeGPT\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"Daoguang/PyCodeGPT\")\n",
    "    return tokenizer, model\n",
    "\n",
    "def generate_code(description, tokenizer, model, max_length=100):\n",
    "    \"\"\"\n",
    "    Generate code from a given description using the provided tokenizer and model.\n",
    "\n",
    "    Args:\n",
    "        description (str): The text description to generate code from.\n",
    "        tokenizer (AutoTokenizer): The tokenizer for encoding the input.\n",
    "        model (AutoModelForCausalLM): The model for generating the code.\n",
    "        max_length (int): The maximum length of the generated code. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated code.\n",
    "    \"\"\"\n",
    "    # Encode the input description\n",
    "    inputs = tokenizer(description, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Generate code from the description\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'], \n",
    "        attention_mask=inputs['attention_mask'], \n",
    "        max_length=max_length, \n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "    # Decode and return the generated code\n",
    "    generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_code\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer, model = load_model()\n",
    "\n",
    "# Example 1: Generate a function to add two numbers\n",
    "description1 = \"Write a Python function to verify if a number is prime\"\n",
    "generated_code1 = generate_code(description1, tokenizer, model)\n",
    "print(\"Example 1 - Generated Code:\")\n",
    "print(generated_code1)\n",
    "\n",
    "# Example 2: Generate a function to find the factorial of a number\n",
    "description2 = \"Write a Python function to find the factorial of a number\"\n",
    "generated_code2 = generate_code(description2, tokenizer, model)\n",
    "print(\"\\nExample 2 - Generated Code:\")\n",
    "print(generated_code2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
